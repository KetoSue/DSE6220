{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8efd322-9399-4dce-a6be-ba76abbfd705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tracking Models with MLflow\n",
    "\n",
    "MLflow is pre-installed on the Databricks Runtime for ML. If you are not using the ML Runtime, you will need to install mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--master local[2] pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "    .builder \n",
    "     .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ceb6cf-eab3-478c-87f0-107759998090",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "filePath = \"data/sf-airbnb-clean.parquet\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, maxDepth=5, \n",
    "                           numTrees=100, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure you have GIT installed before running the MLFlow code\n",
    "https://git-scm.com/download/win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e052e26-1e88-41c5-a7d8-f7bef00ddf6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Open up Anaconda prompt\n",
    "2) Navigate to the DSE6220/Week 8/Examples folder\n",
    "3) Run mlflow ui\n",
    "3) Open any browser and enter: http://localhost:5000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729b4168-2249-4af8-bb4a-4ed45bb985f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "\n",
    "with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "  # Log params: Num Trees and Max Depth\n",
    "  mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "  mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    " \n",
    "  # Log model\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "  # Log metrics: RMSE and R2\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                            labelCol=\"price\")\n",
    "  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "  mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "  # Log artifact: Feature Importance Scores\n",
    "  rfModel = pipelineModel.stages[-1]\n",
    "  pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n",
    "                                    rfModel.featureImportances)), \n",
    "                          columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "  # First write to local filesystem, then tell MLflow where to find that file\n",
    "  pandasDF.to_csv(\"/tmp/feature-importance.csv\", index=False)\n",
    "  mlflow.log_artifact(\"/tmp/feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4d9c2b8-fd42-48ce-8f8a-a3a7dff0ec76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MLflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2937f4e-4b61-4d7c-be5d-e16138ccf829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 0.22794251914574226, 'rmse': 211.5096898777315}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id,\n",
    "                          order_by=[\"attributes.start_time desc\"], \n",
    "                          max_results=1)\n",
    "run_id = runs[0].info.run_id\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fe38d32-cb23-4fa0-9929-85babefd5815",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md ## Generate Batch Predictions\n",
    "\n",
    "Let's load the model back in to generate batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a654bb1a-aa40-4f57-9935-60a40b694cb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/13 13:11:00 INFO mlflow.spark: 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model' resolved as 'file:///C:/Users/Lowhorn/Documents/DSE6220/Week%208/Examples/mlruns/0/11fbfafa15e54a1a940fe4b0aaf5ec74/artifacts/model'\n",
      "2024/04/13 13:11:00 INFO mlflow.spark: URI 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model/sparkml' does not point to the current DFS.\n",
      "2024/04/13 13:11:00 INFO mlflow.spark: File 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model/sparkml' not found on DFS. Will attempt to upload the file.\n",
      "2024/04/13 13:11:00 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/aa308d23-cffb-4d58-8a9b-410eb4d757a1\n"
     ]
    }
   ],
   "source": [
    "# Load saved model with MLflow\n",
    "pipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n",
    "\n",
    "# Generate Predictions\n",
    "inputPath = \"data/sf-airbnb-clean.parquet\"\n",
    "inputDF = spark.read.parquet(inputPath)\n",
    "predDF = pipelineModel.transform(inputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7674bc-879f-44a7-bacc-c2a37fc6cd42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate Streaming Predictions\n",
    "\n",
    "We can do the same thing to generate streaming predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8769ffb0-8ea0-42e3-8c4a-428dc38bc8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/13 13:11:11 INFO mlflow.spark: 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model' resolved as 'file:///C:/Users/Lowhorn/Documents/DSE6220/Week%208/Examples/mlruns/0/11fbfafa15e54a1a940fe4b0aaf5ec74/artifacts/model'\n",
      "2024/04/13 13:11:11 INFO mlflow.spark: URI 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model/sparkml' does not point to the current DFS.\n",
      "2024/04/13 13:11:11 INFO mlflow.spark: File 'runs:/11fbfafa15e54a1a940fe4b0aaf5ec74/model/sparkml' not found on DFS. Will attempt to upload the file.\n",
      "2024/04/13 13:11:11 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/fe0db27d-4bbf-4518-bf91-cc8a333f13e7\n"
     ]
    }
   ],
   "source": [
    "# Load saved model with MLflow\n",
    "pipelineModel = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n",
    "\n",
    "# Set up simulated streaming data\n",
    "repartitionedPath = \"data/sf-airbnb-clean-100p.parquet\"\n",
    "schema = spark.read.parquet(repartitionedPath).schema\n",
    "\n",
    "streamingData = (spark\n",
    "                 .readStream\n",
    "                 .schema(schema) # Can set the schema this way\n",
    "                 .option(\"maxFilesPerTrigger\", 1)\n",
    "                 .parquet(repartitionedPath))\n",
    "\n",
    "# Generate Predictions\n",
    "streamPred = pipelineModel.transform(streamingData)\n",
    "\n",
    "# Uncomment the line below to see the streaming predictions\n",
    "# display(streamPred)\n",
    "\n",
    "# Just remember to stop your stream at the end!\n",
    "# streamPred.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "11-1 MLflow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
